{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Downloaded the data set from github :\n",
        "\n"
      ],
      "metadata": {
        "id": "AQZZPeylwJ7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShwYuJZCyi7E"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_Dataset.zip #Unziping the data set\n",
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "VAoN3czQ0dF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from time import time\n",
        "\n",
        "\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import sequence \n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import LSTM, Embedding , Dense , Activation , Flatten , Reshape , Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "_Qjhdv5V1H3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the token path. token path basicly contains the images and texts. and we can see in same Image there are 5 captions as below.\n"
      ],
      "metadata": {
        "id": "RXHKoMXOyuUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_path = \"Flickr8k.token.txt\"\n",
        "train_images_path = \"Flickr_8k.trainImages.txt\"\n",
        "test_images_path = \"Flickr_8k.testImages.txt\"\n",
        "images_path = \"Flicker8k_Dataset/\"\n",
        "\n",
        "doc = open(token_path,'r').read()\n",
        "print(doc[:410])"
      ],
      "metadata": {
        "id": "EEXXjise4PrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = dict()\n",
        "for line in doc.split('\\n'): # spliting the doc based on new line\n",
        "  tokens = line.split()  # here spliting lines into tokens\n",
        "  if len(line) > 2:\n",
        "    image_id = tokens[0].split('.')[0] #spliting the only image id\n",
        "    image_desc = ' '.join(tokens[1:])  # spliting the decsriptions \n",
        "    if image_id not in descriptions:\n",
        "      descriptions[image_id] = list()\n",
        "    descriptions[image_id].append(image_desc)\n",
        "  \n"
      ],
      "metadata": {
        "id": "m5IiZLhjpj6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have created a dictonary. my dict name is desriptions which has key and values. each key contains name of the image id and value conatins list of captions. \n"
      ],
      "metadata": {
        "id": "682jB55tz_4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions"
      ],
      "metadata": {
        "id": "_gLiEUQTu60U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the text data:"
      ],
      "metadata": {
        "id": "ft4wBjQY3Oo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table = str.maketrans('','',string.punctuation)\n",
        "for key,desc_list in descriptions.items():\n",
        "  for i in range(len(desc_list)):\n",
        "    desc = desc_list[i]\n",
        "    desc = desc.split()\n",
        "    desc = [word.lower() for word in desc]\n",
        "    desc = [w.translate(table) for w in desc ]\n",
        "    desc_list[i] = ' '.join(desc)\n"
      ],
      "metadata": {
        "id": "Lz5PZiyY27ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the one image with its 5 captions\n",
        "pic = '/content/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg'\n",
        "x = plt.imread(pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "descriptions['101669240_b2d3e7f17b']"
      ],
      "metadata": {
        "id": "5MOPLAGl5Qbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting the number of unique word or vlocabulary:"
      ],
      "metadata": {
        "id": "rXxpczFg9MyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set()\n",
        "for key in descriptions.keys():\n",
        "  [vocabulary.update(d.split()) for d in descriptions[key]]\n",
        "\n",
        "print('Original Vocabulary Size : %d' % len(vocabulary))"
      ],
      "metadata": {
        "id": "EuscjZCA55UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines= list()\n",
        "for key, desc_list in descriptions.items():\n",
        "  for desc in desc_list:\n",
        "    lines.append(key + ' '+ desc)\n",
        "new_descriptins = '\\n'.join(lines)\n",
        "new_descriptins"
      ],
      "metadata": {
        "id": "8HKLKr909KZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we have made some changes. and created new descriptions. since  description contains key and value but that is not enough hence we have \n",
        "to make another variable which will contain seprate seprate description against same image id."
      ],
      "metadata": {
        "id": "9PSz6yNs-WQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = open(train_images_path,'r').read()\n",
        "dataset = list()\n",
        "for line in doc.split('\\n'):\n",
        "  if len(line) > 1:\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "\n",
        "\n",
        "train = set(dataset)\n",
        "train"
      ],
      "metadata": {
        "id": "1gWxlBLc-Ulx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Above we are creatimg the set which will be containg the all id's for traing. here we have removed extaintion \"jpg\".*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "T-DrRUVfAwQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Downlaoding the glove embeddings:\n",
        "\n",
        "1.   Instead of creatimg embedding from scratc we are using glove embedding as\n",
        "it has larger value and it understand the value in better way as compare to small embaddings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0rhbIP4uBPCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "1ahIXAByAm9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "MmQZDiDBBJyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = glob.glob(images_path+'*.jpg')\n",
        "train_images = set(open(train_images_path,'r').read().strip().split('\\n'))\n",
        "train_img =[]\n",
        "for i in img:\n",
        "  if i[len(images_path):] in train_images:\n",
        "    train_img.append(i)\n",
        "\n",
        "print(train_img) # its path of train and test images\n",
        "test_images = set(open(test_images_path,'r').read().strip().split('\\n'))\n",
        "test_img =[]\n",
        "for i in img:\n",
        "  if i[len(images_path):] in test_images:\n",
        "    test_img.append(i)"
      ],
      "metadata": {
        "id": "EYonxcaUCqwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are adding the \"start\" and \"end\" token for each sentence. where \"startseq\" tell us where tio start and \"endseq\" tell us where to end the whole things. as soos as our model predicts end we know this time we have to stop and we can not pass the value to next LSTM. "
      ],
      "metadata": {
        "id": "Sh74z529jg52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_descriptions = dict()\n",
        "for line in new_descriptins.split('\\n'):\n",
        "  tokens = line.split()\n",
        "  image_id,image_desc = tokens[0],tokens[1:]\n",
        "  if image_id in train:\n",
        "    if image_id not in train_descriptions:\n",
        "      train_descriptions[image_id] = list()\n",
        "    desc = 'startseq ' + ' '.join(image_desc)+' endseq'\n",
        "    train_descriptions[image_id].append(desc)"
      ],
      "metadata": {
        "id": "xXPbOpt-FMm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_descriptions"
      ],
      "metadata": {
        "id": "k688IVzxGaNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_captions = []\n",
        "for key,val in train_descriptions.items():\n",
        "  for cap in val:\n",
        "    all_train_captions.append(cap)"
      ],
      "metadata": {
        "id": "V3nQHyGRNJsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_captions"
      ],
      "metadata": {
        "id": "eJLecbmHOKJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the vocabualty count, if any word occuring more then 10 times or more then 10 times, only that case only adding to vocabulary\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "  nsents+=1\n",
        "  for w in sent.split(' '):\n",
        "    word_counts[w] = word_counts.get(w,0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >=word_count_threshold]\n",
        "\n",
        "print('vocabulary = %d' % (len(vocab)))"
      ],
      "metadata": {
        "id": "I3Thmo9YO_KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix =1\n",
        "\n",
        "for w in vocab:\n",
        "  wordtoix[w] = ix\n",
        "  ixtoword[ix] = w\n",
        "  ix +=1\n",
        "  \n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ],
      "metadata": {
        "id": "48Hms0TjQBlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here above instead using any tokenoizer we are creating index of each word."
      ],
      "metadata": {
        "id": "XZydPcybR-nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ixtoword"
      ],
      "metadata": {
        "id": "NwlktwHmRFs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here finding the max length of captions\n",
        "all_desc = list()\n",
        "for key in train_descriptions.keys():\n",
        "  [all_desc.append(d) for d in train_descriptions[key]]\n",
        "lines = all_desc\n",
        "max_length = max(len(d.split())for d in lines)\n",
        "\n",
        "print(\"Description length : %d\" % max_length)"
      ],
      "metadata": {
        "id": "IWrTZU2cSpzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are loading the glove model and using glove file which has words or key- word desct."
      ],
      "metadata": {
        "id": "fGl31kIemnZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('glove.6B.200d.txt'),encoding=\"utf-8\")\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:],dtype='float32')\n",
        "  embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "RAmB9O_STcyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size,embedding_dim)) # creatimg embedding vector \n",
        "for word, i in wordtoix.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  "
      ],
      "metadata": {
        "id": "9iy4CKEKmg-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix"
      ],
      "metadata": {
        "id": "JfmzKKIwoNI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the basic CNN mode - Inception**"
      ],
      "metadata": {
        "id": "NckPun4DqR6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionV3(weights='imagenet')"
      ],
      "metadata": {
        "id": "Ab30jEvXo_ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since inception is classification model. this will be use as a classification but here we do not need clasification, we just need to extarct the features hence we are using the all the layers except the fully connected layers. and it will give all the features.\n"
      ],
      "metadata": {
        "id": "EtPt_qx8tj1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_new = Model(model.input,model.layers[-2].output)"
      ],
      "metadata": {
        "id": "_07pdkuNqEPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess for images**"
      ],
      "metadata": {
        "id": "9khMJvDWu-dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path):\n",
        "  img = image.load_img(image_path,target_size=(299,299)) # loadinmg the image\n",
        "  x = image.img_to_array(img) # conbverting into array\n",
        "  x = np.expand_dims(x,axis=0) # expending the dimention because we are adding one more axis too it\n",
        "  x = preprocess_input(x) # its keras function which we have imported above \n",
        "  return x"
      ],
      "metadata": {
        "id": "YAsavoKEsdC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "z-9VvU4GyLf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# encoding the imgae where we will get encoded vectors\n",
        "def encode(image):\n",
        "  image = preprocess(image)\n",
        "  fea_vec =  model_new.predict(image)\n",
        "  fea_vec = np.reshape(fea_vec,fea_vec.shape[1])\n",
        "  return fea_vec\n",
        "\n",
        "\n",
        "# doing for train\n",
        "\n",
        "encoding_train = {}\n",
        "for img in train_img:\n",
        "  encoding_train[img[len(images_path):]] = encode(img)\n",
        "train_features = encoding_train\n",
        "\n",
        "# doing for test\n",
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "  encoding_test[img[len(images_path):]] = encode(img)\n"
      ],
      "metadata": {
        "id": "CLczljy4u1tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the bottelneck tarin features\n",
        "with open(\"/content/drive/MyDrive/encoded_train_images.pkl\",\"wb\") as encoded_pickle:\n",
        "  pickle.dump(encoding_train,encoded_pickle)"
      ],
      "metadata": {
        "id": "NJm09pswywbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the bottelneck test features\n",
        "with open(\"/content/drive/MyDrive/encoded_test_images.pkl\",\"wb\") as encoded_pickle:\n",
        "  pickle.dump(encoding_test,encoded_pickle)"
      ],
      "metadata": {
        "id": "HLktnXnJz38F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the LSTM model \n",
        "inputs1 = Input(shape=(2048,)) # if we remove 2 last layers from inceptionv3 then the remaining layers would be 2048  \n",
        "fe1 = Dropout(0.5)(inputs1) # droupout layers\n",
        "fe2 = Dense(256, activation='relu')(fe1) # readucing 2048 to 256\n",
        "\n",
        "# now passing through embedding layer\n",
        "inputs2 = Input(shape=(max_length,)) \n",
        "se1 = Embedding(vocab_size,embedding_dim,mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "decoder1 = add([fe2,se3]) # the value here which we got from CNN and LSTM , now we ware adding them\n",
        "decoder2 = Dense(256,activation='relu')(decoder1) # and passing from another dense layer\n",
        "outputs = Dense(vocab_size,activation='softmax')(decoder2) # passing through sofmax layer\n",
        "\n",
        "model = Model(inputs=[inputs1,inputs2], outputs=outputs) \n",
        "model.summary()"
      ],
      "metadata": {
        "id": "A0xpBcuG0hTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ],
      "metadata": {
        "id": "Wieuo_vh0he8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam')"
      ],
      "metadata": {
        "id": "gccc-q7w0hhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(descriptions,photos,wordtoix,max_length,num_photos_per_batch):\n",
        "  X1,X2,y=list(),list(),list()\n",
        "  n = 0\n",
        "  #loop for ever over images\n",
        "  while 1:\n",
        "    for key,desc_list in descriptions.items():\n",
        "      n +=1\n",
        "      # retrive the photo feature\n",
        "      photo = photos[key +'.jpg']\n",
        "      for desc in desc_list:\n",
        "        #encode the sequence\n",
        "        seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "        # split one sequence into multipal X,y pairs\n",
        "        for i in range(1,len(seq)):\n",
        "          #split into input and output pair\n",
        "          in_seq,out_seq=seq[:1],seq[i]\n",
        "          #paid input sequence\n",
        "          in_seq = pad_sequences([in_seq],maxlen=max_length)[0]\n",
        "          #encode output sequence\n",
        "          out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
        "          #store\n",
        "          X1.append(photo)\n",
        "          X2.append(in_seq)\n",
        "          y.append(out_seq)\n",
        "\n",
        "\n",
        "\n",
        "      if n==num_photos_per_batch:\n",
        "        yield ([array(X1),array(X2)],array(y))\n",
        "        X1,X2,y=list(),list(),list()\n",
        "        n=0\n"
      ],
      "metadata": {
        "id": "YBgl2M-x0hjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 3\n",
        "steps = len(train_descriptions)//batch_size\n",
        "\n",
        "generator = data_generator(train_descriptions,train_features,wordtoix,max_length,batch_size)\n",
        "model.fit(generator,epochs=epochs,steps_per_epoch=steps,verbose=1)"
      ],
      "metadata": {
        "id": "q-DuRVVf4FlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('content/drive/MyDrive/model.h5')"
      ],
      "metadata": {
        "id": "UjR9qxzuJZeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('content/drive/MyDrive/model_30.h5')"
      ],
      "metadata": {
        "id": "f1HnAjWWJq7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedysearch(photo):\n",
        "  in_text = 'startseq'\n",
        "  for i in range(max_length):\n",
        "    sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "    sequence = pad_sequences([sequence],maxlen=max_length)\n",
        "    yhat = model.predict([photo,sequence],verbose=0)\n",
        "    yhat = np.argmax(yhat)\n",
        "    word = ixtoword[yhat]\n",
        "    in_text += ' '+word\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "\n",
        "  final = in_text.split()\n",
        "  final = final[1:-1]\n",
        "  final = ' '.join(final)\n",
        "  return final\n"
      ],
      "metadata": {
        "id": "WVmOgBxbJ6vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_predictions(image,beam_index =3):\n",
        "  start = [wordtoix[\"startseq\"]]\n",
        "  start_word = [[start,0.0]]\n",
        "  while len(start_word[0][0]) < max_length:\n",
        "    temp = []\n",
        "    for s in start_word:\n",
        "      per_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "      preds = model.predict([image,per_caps],verbose=0)\n",
        "      word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "      #getting the top <beam_index>(n) predictions and creating a\n",
        "      #new list so as to put them via the model again\n",
        "      for w in word_preds:\n",
        "        next_cap,prob=s[0][:],s[1]\n",
        "        next_cap.append(w)\n",
        "        temp.append([next_cap,prob])\n",
        "\n",
        "    start_word = temp\n",
        "    #sorting according to the probabilities\n",
        "    start_word = sorted(star_word,reverse=Flase,key=lambda l : l[1])\n",
        "    # getting the top words\n",
        "    start_word = start_word[-beam_index:]\n",
        "  start_word = start_word[-1][0]\n",
        "  intermediate_caption = [ixtoword[i] for i in start_word]\n",
        "  final_caption = []\n",
        "\n",
        "  for i in intermediate_caption:\n",
        "    if i != 'endseq':\n",
        "      final_caption.append(i)\n",
        "    else:\n",
        "      break\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "  final_caption = ' '.join(final_caption[1:])\n",
        "  return final_caption\n"
      ],
      "metadata": {
        "id": "cF4NGoS_L4_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pic = '/content/Flicker8k_Dataset/101669240_b2d3e7f17b.jpg'\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x = plt.imread(images_path+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Gready search:\", greedysearch(image))\n",
        "print(\"Beam Search, K =3:\",beam_search_predictions(image,beam_index=3))\n",
        "print(\"Beam Search, K =5:\",beam_search_predictions(image,beam_index=5))\n",
        "print(\"Beam Search, K =7:\",beam_search_predictions(image,beam_index=7))\n",
        "print(\"Beam Search, K =10:\",beam_search_predictions(image,beam_index=10))"
      ],
      "metadata": {
        "id": "lGvL-pVUWT2Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}